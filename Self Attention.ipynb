{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Self Attention_Rev2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5EwavORoKgGl",
        "S_fxrq0ZsJiQ",
        "foK4d8faNfgW",
        "Vm6XGWGfkL5f",
        "9Ry7S5fvYkvp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqTaJ7zOpf7C",
        "colab_type": "code",
        "outputId": "7ff2e550-934e-4446-8347-b475f8a2c7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Input, Dot, Activation, Conv1D, LayerNormalization\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed, RepeatVector, Add, concatenate\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naK_gKSNxOs-",
        "colab_type": "text"
      },
      "source": [
        "**Quick Instruction Note**\n",
        "\n",
        "1.   The input and ouput both are sequences of 5 items. The first two items of the ouput are the same as the input: input = [37,38,13,3,11], output = [37,38,0,0,0]\n",
        "2.   The input and output are one-hot encoded and so the input shape of the model is (batch_size, timesteps=5, feature=50) since there are 5 items and the item values are from 0 to 50\n",
        "3.   Since the model is seq2seq, thus the attention map's size is (timesteps*timesteps) and it is generated using query and key. The attention map is then multipled with the value to get the content vector\n",
        "4.   https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EwavORoKgGl",
        "colab_type": "text"
      },
      "source": [
        "# **Original Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-g5Y955WDYt",
        "colab_type": "code",
        "outputId": "0b27236f-1867-4612-8b4d-b68c6a8bf8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# define the encoder-decoder model\n",
        "def baseline_model(n_timesteps_in, n_features):\n",
        "  input_ = Input(shape=(n_timesteps_in, n_features))\n",
        "  x = LSTM(150)(input_)\n",
        "  x = RepeatVector(n_timesteps_in)(x)\n",
        "  x = LSTM(150, return_sequences=True)(x)\n",
        "  output_ = TimeDistributed(Dense(n_features, activation=\"softmax\"))(x)\n",
        "  model = Model(inputs=input_, outputs=output_)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t#model = Sequential()\n",
        "\t#model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
        "\t#model.add(RepeatVector(n_timesteps_in))\n",
        "\t#model.add(LSTM(150, return_sequences=True))\n",
        "\t#model.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
        "  return model\n",
        "\n",
        "# My own attention model\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        "  input_ = Input(shape=(n_timesteps_in, n_features))\n",
        "  x = LSTM(150, return_sequences=True)(input_)\n",
        "\n",
        "  # Create query, key, value for self-attention computation\n",
        "  # Conv1D with kernel_size 1 is equal to Dense\n",
        "  query = Conv1D(filters=32, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  key = Conv1D(filters=32, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  value = Conv1D(filters=64, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  # Dot product query and key - the query_key's size should be (timesteps * timesteps)\n",
        "  # axes: indicate the axes to multiply with the other tensor\n",
        "  query_key = Dot(axes=2, normalize=True)([query,key]) \n",
        "  # Softmax is applied across the columns to find the attention of one timestep to the others\n",
        "  # The attention's size should be (timesteps * timesteps)\n",
        "  attention = Activation(\"softmax\")(query_key)\n",
        "  content_vector = Dot(axes=1, normalize=True, name=\"reg_dot2\")([attention,value])\n",
        "\n",
        "  output_ = Dense(n_features, activation=\"softmax\")(content_vector)\n",
        "  model = Model(inputs=input_, outputs=output_)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        " \n",
        "# train and evaluate a model, return accuracy\n",
        "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs):\n",
        "\t# train LSTM\n",
        "\tfor epoch in range(epochs):\n",
        "\t\t# generate new random sequence\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\t# fit model for one epoch on this sequence\n",
        "\t\tmodel.fit(X, y, epochs=1, verbose=0)\n",
        "\t# evaluate LSTM\n",
        "\ttotal, correct = 100, 0\n",
        "\tfor _ in range(total):\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\tyhat = model.predict(X, verbose=0)\n",
        "\t\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\t\tcorrect += 1\n",
        "\treturn float(correct)/float(total)*100.0\n",
        " \n",
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 10\n",
        "epochs = 20000\n",
        "# evaluate encoder-decoder model\n",
        "print('Encoder-Decoder Model')\n",
        "results = list()\n",
        "\n",
        "model = baseline_model(n_timesteps_in, n_features)\n",
        "model.summary()\n",
        "#accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "#results.append(accuracy)\n",
        "#print(accuracy)\n",
        "\n",
        "model = attention_model(n_timesteps_in, n_features)\n",
        "model.summary()\n",
        "#accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "#results.append(accuracy)\n",
        "#print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder Model\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 5, 50)             0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 150)               120600    \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 5, 150)            0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 5, 150)            180600    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 5, 50)             7550      \n",
            "=================================================================\n",
            "Total params: 308,750\n",
            "Trainable params: 308,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 5, 50)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 5, 150)       120600      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 5, 32)        4832        lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 5, 32)        4832        lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 5, 5)         0           conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 5, 5)         0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 5, 64)        9664        lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reg_dot2 (Dot)                  (None, 5, 64)        0           activation_3[0][0]               \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 5, 50)        3250        reg_dot2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 143,178\n",
            "Trainable params: 143,178\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vU2BdqHgLTDJ"
      },
      "source": [
        "**5000 Epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5420096d-23a8-4f74-9e8a-21f9d85d8933",
        "id": "x-cERys3LTDP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 5\n",
        "epochs = 5000\n",
        "\n",
        "# evaluate encoder-decoder model\n",
        "print('Encoder-Decoder Model')\n",
        "results = list()\n",
        "for _ in range(n_repeats):\n",
        "  model = baseline_model(n_timesteps_in, n_features)\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%% \\n' % (sum(results)/float(n_repeats)))\n",
        "\n",
        "# evaluate encoder-decoder with attention model\n",
        "print('Encoder-Decoder With Attention Model')\n",
        "results = list()\n",
        "for _ in range(n_repeats):\n",
        "  model = attention_model(n_timesteps_in, n_features)\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder Model\n",
            "24.0\n",
            "22.0\n",
            "23.0\n",
            "22.0\n",
            "19.0\n",
            "Mean Accuracy: 22.00% \n",
            "\n",
            "Encoder-Decoder With Attention Model\n",
            "36.0\n",
            "47.0\n",
            "47.0\n",
            "73.0\n",
            "92.0\n",
            "Mean Accuracy: 59.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_fxrq0ZsJiQ",
        "colab_type": "text"
      },
      "source": [
        "# **Revision Self Attention Model 1 (5000 epochs)**\n",
        "\n",
        "1.   Adjust the hidden neurons number of query, key and value's layers\n",
        "2.   Performance improves by double and is more stable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cpKyoFrZP_U",
        "colab_type": "code",
        "outputId": "4efa77a2-0653-4201-e650-835379dea860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# train and evaluate a model, return accuracy\n",
        "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs):\n",
        "\t# train LSTM\n",
        "\tfor epoch in range(epochs):\n",
        "\t\t# generate new random sequence\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\t# fit model for one epoch on this sequence\n",
        "\t\tmodel.fit(X, y, epochs=1, verbose=0)\n",
        "\t# evaluate LSTM\n",
        "\ttotal, correct = 100, 0\n",
        "\tfor _ in range(total):\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\tyhat = model.predict(X, verbose=0)\n",
        "\t\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\t\tcorrect += 1\n",
        "\treturn float(correct)/float(total)*100.0\n",
        "\n",
        "# My own attention model\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        "  input_ = Input(shape=(n_timesteps_in, n_features))\n",
        "  x = LSTM(150, return_sequences=True)(input_)\n",
        "\n",
        "  # Create query, key, value for self-attention computation\n",
        "  # Conv1D with kernel_size 1 is equal to Dense\n",
        "  query = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  key = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  value = Conv1D(filters=150, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        "  # Dot product query and key - the query_key's size should be (timesteps * timesteps)\n",
        "  # axes: indicate the axes to multiply with the other tensor\n",
        "  query_key = Dot(axes=2, normalize=True)([query,key]) \n",
        "  # Softmax is applied across the columns to find the attention of one timestep to the others\n",
        "  # The attention's size should be (timesteps * timesteps)\n",
        "  attention = Activation(\"softmax\")(query_key)\n",
        "  content_vector = Dot(axes=1, normalize=True, name=\"reg_dot2\")([attention,value])\n",
        "\n",
        "  output_ = Dense(n_features, activation=\"softmax\")(content_vector)\n",
        "  model = Model(inputs=input_, outputs=output_)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 5\n",
        "epochs = 5000\n",
        "\n",
        "# evaluate encoder-decoder with attention model\n",
        "print('Encoder-Decoder With Attention Model')\n",
        "results = list()\n",
        "for i in range(n_repeats):\n",
        "  model = attention_model(n_timesteps_in, n_features)\n",
        "  if i == 0:\n",
        "    model.summary()\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder With Attention Model\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 5, 150)       120600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 5, 75)        11325       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 5, 75)        11325       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 5, 5)         0           conv1d_3[0][0]                   \n",
            "                                                                 conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 5, 5)         0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 5, 150)       22650       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reg_dot2 (Dot)                  (None, 5, 150)       0           activation_1[0][0]               \n",
            "                                                                 conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5, 50)        7550        reg_dot2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 173,450\n",
            "Trainable params: 173,450\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "62.0\n",
            "86.0\n",
            "86.0\n",
            "99.0\n",
            "79.0\n",
            "Mean Accuracy: 82.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H2PQ6ReSOK5X"
      },
      "source": [
        "**Activation Map Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "de5be291-b233-4368-bb7e-94bbd62413c3",
        "id": "rB94icpGOK5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "# For revision model 1\n",
        "activation_model = Model(inputs=model.input, outputs=model.layers[-4].output, name=\"Activation_model\")\n",
        "activation_model.summary()\n",
        "\n",
        "X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "activation = activation_model.predict(X, verbose=0)\n",
        "yhat = model.predict(X, verbose=0)\n",
        "\n",
        "fig = plt.matshow(activation[0].transpose())\n",
        "plt.colorbar(fig)\n",
        "plt.show()\n",
        "\n",
        "one_hot_decode(X[0]), one_hot_decode(y[0]), one_hot_decode(yhat[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Activation_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 5, 150)       120600      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 5, 75)        11325       lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 5, 75)        11325       lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 5, 5)         0           conv1d_15[0][0]                  \n",
            "                                                                 conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5, 5)         0           dot_5[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 143,250\n",
            "Trainable params: 143,250\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAADvCAYAAADhAiFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKiYJIlLFJEFEqpgkiEgVkwQRqfr/FeUV2lYKEXYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([13, 46, 49, 0, 40], [13, 46, 0, 0, 0], [13, 46, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foK4d8faNfgW",
        "colab_type": "text"
      },
      "source": [
        "# **Revision Self Attention Model 2 (5000 epochs)**\n",
        "\n",
        "1.   Adjust the hidden neurons number of query, key and value's layers\n",
        "2.   Add the original input to the attention layer with the attention output (attention map)\n",
        "3.   The result is slighty better than that of the revision model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX2hssQXrIN0",
        "colab_type": "code",
        "outputId": "0722fa02-71e1-429a-c478-6d825409ca9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# My own attention model\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        " input_ = Input(shape=(n_timesteps_in, n_features))\n",
        " x = LSTM(150, return_sequences=True)(input_)\n",
        "\n",
        " # Create query, key, value for self-attention computation\n",
        " # Conv1D with kernel_size 1 is equal to Dense\n",
        " query = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " key = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " value = Conv1D(filters=150, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " # Dot product query and key - the query_key's size should be (timesteps * timesteps)\n",
        " # axes: indicate the axes to multiply with the other tensor\n",
        " query_key = Dot(axes=2, normalize=True)([query,key]) \n",
        " # Softmax is applied across the columns to find the attention of one timestep to the others\n",
        " # The attention's size should be (timesteps * timesteps)\n",
        " attention = Activation(\"softmax\")(query_key)\n",
        " content_vector = Dot(axes=1, normalize=True, name=\"reg_dot2\")([attention,value])\n",
        " content_vector = Add()([x,content_vector])\n",
        " output_ = Dense(n_features, activation=\"softmax\")(content_vector)\n",
        "\n",
        " model = Model(inputs=input_, outputs=output_)\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 10\n",
        "epochs = 5000\n",
        "\n",
        "# evaluate encoder-decoder with attention model\n",
        "print('Encoder-Decoder With Attention Model')\n",
        "results = list()\n",
        "for i in range(n_repeats):\n",
        "  model = attention_model(n_timesteps_in, n_features)\n",
        "  if i == 0:\n",
        "    model.summary()\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder With Attention Model\n",
            "Model: \"model_50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_50 (InputLayer)           (None, 5, 50)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_53 (LSTM)                  (None, 5, 150)       120600      input_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_139 (Conv1D)             (None, 5, 75)        11325       lstm_53[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_140 (Conv1D)             (None, 5, 75)        11325       lstm_53[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_47 (Dot)                    (None, 5, 5)         0           conv1d_139[0][0]                 \n",
            "                                                                 conv1d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 5, 5)         0           dot_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_141 (Conv1D)             (None, 5, 150)       22650       lstm_53[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reg_dot2 (Dot)                  (None, 5, 150)       0           activation_47[0][0]              \n",
            "                                                                 conv1d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 5, 150)       0           lstm_53[0][0]                    \n",
            "                                                                 reg_dot2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 5, 50)        7550        add_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 173,450\n",
            "Trainable params: 173,450\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "71.0\n",
            "94.0\n",
            "91.0\n",
            "100.0\n",
            "79.0\n",
            "94.0\n",
            "73.0\n",
            "98.0\n",
            "83.0\n",
            "96.0\n",
            "Mean Accuracy: 87.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QzvP36CxRiYa"
      },
      "source": [
        "**Activation Map Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TG7on62oRiYk",
        "outputId": "2843b1e4-12a9-4d26-afb9-9d53a36cd8c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "source": [
        "# For revision model 2\n",
        "activation_model = Model(inputs=model.input, outputs=model.layers[-5].output, name=\"Activation_model\")\n",
        "activation_model.summary()\n",
        "\n",
        "X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "activation = activation_model.predict(X, verbose=0)\n",
        "yhat = model.predict(X, verbose=0)\n",
        "\n",
        "fig = plt.matshow(activation[0])\n",
        "plt.colorbar(fig)\n",
        "plt.show()\n",
        "\n",
        "one_hot_decode(X[0]), one_hot_decode(y[0]), one_hot_decode(yhat[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Activation_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_59 (InputLayer)           (None, 5, 50)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_62 (LSTM)                  (None, 5, 150)       120600      input_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_166 (Conv1D)             (None, 5, 75)        11325       lstm_62[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_167 (Conv1D)             (None, 5, 75)        11325       lstm_62[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_56 (Dot)                    (None, 5, 5)         0           conv1d_166[0][0]                 \n",
            "                                                                 conv1d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 5, 5)         0           dot_56[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 143,250\n",
            "Trainable params: 143,250\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAADzCAYAAACVFuOvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([23, 4, 48, 29, 12], [23, 4, 0, 0, 0], [23, 4, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm6XGWGfkL5f",
        "colab_type": "text"
      },
      "source": [
        "# **Revision Self Attention Model 3 (5000 epochs)**\n",
        "\n",
        "1.   Adjust the hidden neurons number of query, key and value's layers\n",
        "2.   Add the original input to the attention layer with the attention output (attention map)\n",
        "3.   Add the layer normalization\n",
        "4.   Best result among the three models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7em_VM_V0H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# train and evaluate a model, return accuracy\n",
        "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs):\n",
        "\t# train LSTM\n",
        "\tfor epoch in range(epochs):\n",
        "\t\t# generate new random sequence\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\t# fit model for one epoch on this sequence\n",
        "\t\tmodel.fit(X, y, epochs=1, verbose=0)\n",
        "\t# evaluate LSTM\n",
        "\ttotal, correct = 100, 0\n",
        "\tfor _ in range(total):\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\tyhat = model.predict(X, verbose=0)\n",
        "\t\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\t\tcorrect += 1\n",
        "\treturn float(correct)/float(total)*100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15685838-378f-4535-d9d8-5f1c50146f91",
        "id": "XtZCqjPtViTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "# My own attention model\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        " input_ = Input(shape=(n_timesteps_in, n_features))\n",
        " x = LSTM(150, return_sequences=True)(input_)\n",
        "\n",
        " # Create query, key, value for self-attention computation\n",
        " # Conv1D with kernel_size 1 is equal to Dense\n",
        " query = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " key = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " value = Conv1D(filters=150, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " # Dot product query and key - the query_key's size should be (timesteps * timesteps)\n",
        " # axes: indicate the axes to multiply with the other tensor\n",
        " query_key = Dot(axes=2, normalize=True)([query,key]) \n",
        " # Softmax is applied across the columns to find the attention of one timestep to the others\n",
        " # The attention's size should be (timesteps * timesteps)\n",
        " attention = Activation(\"softmax\")(query_key)\n",
        " attention = LayerNormalization()(attention)\n",
        " content_vector = Dot(axes=1, normalize=True, name=\"reg_dot2\")([attention,value])\n",
        " content_vector = Add()([x,content_vector])\n",
        " output_ = Dense(n_features, activation=\"softmax\")(content_vector)\n",
        "\n",
        " model = Model(inputs=input_, outputs=output_)\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 5\n",
        "epochs = 5000\n",
        "\n",
        "# evaluate encoder-decoder with attention model\n",
        "print('Encoder-Decoder With Attention Model')\n",
        "results = list()\n",
        "for i in range(n_repeats):\n",
        "  model = attention_model(n_timesteps_in, n_features)\n",
        "  if i == 0:\n",
        "    model.summary()\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder With Attention Model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 5, 150)       120600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 5, 75)        11325       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 5, 75)        11325       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 5, 5)         0           conv1d[0][0]                     \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 5, 5)         0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 5, 5)         10          activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 5, 150)       22650       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "reg_dot2 (Dot)                  (None, 5, 150)       0           layer_normalization[0][0]        \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 5, 150)       0           lstm[0][0]                       \n",
            "                                                                 reg_dot2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 5, 50)        7550        add[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 173,460\n",
            "Trainable params: 173,460\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "100.0\n",
            "99.0\n",
            "100.0\n",
            "100.0\n",
            "100.0\n",
            "Mean Accuracy: 99.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "16Q_pimBViTq"
      },
      "source": [
        "**Activation Map Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e0aed42-c0fe-4ff0-efd3-3d1717d92288",
        "id": "Tv5i8sEhViTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "# For revision model 3\n",
        "activation_model = Model(inputs=model.input, outputs=model.layers[-5].output, name=\"Activation_model\")\n",
        "activation_model.summary()\n",
        "\n",
        "X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "activation = activation_model.predict(X, verbose=0)\n",
        "yhat = model.predict(X, verbose=0)\n",
        "\n",
        "fig = plt.matshow(activation[0])\n",
        "plt.colorbar(fig)\n",
        "plt.show()\n",
        "\n",
        "one_hot_decode(X[0]), one_hot_decode(y[0]), one_hot_decode(yhat[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Activation_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 5, 150)       120600      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 5, 75)        11325       lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 5, 75)        11325       lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, 5, 5)         0           conv1d_12[0][0]                  \n",
            "                                                                 conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 5, 5)         0           dot_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_4 (LayerNor (None, 5, 5)         10          activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 143,260\n",
            "Trainable params: 143,260\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAADtCAYAAABzhPpbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdiElEQVR4nO3de1RU19038O/MKCEGBfGC4KA0rkiJ\n1qR5rFYTW8ULtkHF+rhI1JjldVU0TbVNwMZ6TWrx6etbL6g1MbpcNdZaI62GKBJXEsXG+KReosTL\n6yMqMOgjl5J4d85+/0AmjgKzj/tshhm+n6yzAsyefX7j5ec+e5+zfzYhhAARkQK7vwMgosDHREJE\nyphIiEgZEwkRKWMiISJlTCREpKyZvwMgCjbCcMNmd1jeb2ZmJnbv3o3i4mLs2LEDXbt2faBNVlYW\ncnJyYLfb0bx5c8ycORP9+vUDAGRkZODAgQNo3bo1AGDo0KGYNm2aJbExkRBZzGZ34E7FRQj3Hd9t\nHc3QrHWsVL8DBw7E+PHjMXbs2Drb9OjRAxMnTsSjjz6KkydPYty4cdi/fz9CQ0MBAFOnTsW4cePk\nPogJTCREGog7twDDdyIRwpDus2fPnj7b1Iw+ACA+Ph5CCFRWVqJDhw7S53kYjX6O5Ny5c0hNTUVS\nUhJSU1NRWFjot1gyMzORmJiI+Ph4nD592m9xAEBFRQWmTJmCpKQkDBs2DDNmzEB5eblfY0pLS8Pw\n4cORkpKCMWPG4KuvvvJrPACwcuVK//x+CQEYhu/j7o3lLpcLRUVFXkdVVZVSCNnZ2ejUqZNXElm/\nfj2GDRuGtLQ0nD17Vql/L6KRe+mll0R2drYQQojs7Gzx0ksv+S2WQ4cOiZKSEjFgwABx6tQpv8Uh\nhBAVFRXis88+83z/+9//XsyePduPEQlRVVXl+XrPnj0iJSXFj9EIcfz4cTFp0iS//H7dLPlK3Lx4\nzPdR8pUQQogBAwaIrl27eh3Lly+vtW+Zz3Pw4EHx4x//WJw9e9bzs9LSUuF2u4UQQmzfvl30799f\n3Llzx5LP26hHJGVlZSgoKEBycjIAIDk5GQUFBX77l7dnz56Ijo72y7nvFxERgd69e3u+f/rpp1FS\nUuLHiICWLVt6vv7mm29gs9n8FsutW7ewcOFCzJ8/3z8BCEP+ALBp0yZ89NFHXsfLL7/8UKc+fPgw\nXnvtNWRlZeHxxx/3/DwqKgp2e/Vf+ZSUFFy7dg2lpaXqnxWNfI7E5XIhKioKDkf1DLjD4UD79u3h\ncrkQGRnp5+gaD8MwsHnzZiQmJvo7FLzxxhvIz8+HEALvvPOO3+JYtmwZhg8fDqfT6Z8ADAMw3L7b\n2ar/bFv1D9SxY8cwc+ZMLF++HN26dfN67dKlS4iKigIA7Nu3D3a73fO9qkadSEjOokWL0KJFCy2z\n8Wa99dZbAKqvz5csWYK33367wWM4fPgwjh8/jl//+tcNfu5vfTva8NlO0ptvvonc3FxcuXIFEyZM\nQEREBD744ANMmTIFv/jFL/C9730PCxYswI0bNzB37lzP+5YsWYL4+Hikp6ejrKwMNpsNYWFhWL16\nNZo1syYF2IRovNsIlJWVISkpCQcPHoTD4YDb7Ubv3r2Rm5vr1xFJYmIi1qxZU+s6fkPLzMzEqVOn\nsGbNGoSEhPg7HC89evTAJ5984rlvoaGsXbsWGzdu9Px6lJaWok2bNli8eDGee+65Bonh1oUjwJ1b\nvhs2C0FIp6f1B6RZo54jadOmDRISErBz504AwM6dO5GQkMDLmruWLl2K48ePIysry+9J5OrVq3C5\nXJ7v9+7di/DwcERERDR4LFOnTsX+/fuxd+9e7N27Fx06dMC6desaLIkAgBACQhgSR6P9d9yURj0i\nAYCzZ88iIyMDVVVVaNWqFTIzM70mkBrSvUPL1q1be4aW/nDmzBkkJycjLi7Oc7OR0+lEVlaWX+K5\ncuUK0tLScP36ddjtdoSHhyM9Pf2B63R/8McI8ua5L4A7N303bPYIHvnOf+gPSLNGn0iIAtHNs58B\ntyUSSfNH8EiXH+oPSDNOthLpIITcZGuQ/DvOREKkQ82dqzLtggATCZEOQnL518SzNo0ZEwmRDk1s\nRNKol39rVFVVYcWKFcoPMQVbLADjabSxCANCuH0ewTIiCZhEsnLlykbzh7OxxAIwnsYbi+xzNsGR\nSHhpQ6SD2w1IbGwEt8TzOAGAiYRIB8Mt99CeTJsAwERCpANXbRofh8OBjh07erYTYCzfYjyNNBYh\nuWoTJImEt8gTaXDzX/+AuHnVZzvbI4/hkWeGN0BEejXIiOR2RZHytWDzNp1xu+y8cixLh/9BuY/0\n/BXIfPYV5X4A4LPb/6vcx/bP/4KRvV5Q7qf0ZoVyHwDwz6O70eepJOV+rsk89ObD0RMf46lu/ZX6\niInpgA/3/MXcm5rYfSQNc2ljuKV21Pbdj3oflUVX1OOwsJ/S29ZsdVdapN5P0Y0yCyK529dF9W0f\nr96+YUEkwMULxZb0YwoTCRGpEsYdCPdt3w2t+Ae2EWAiIdKBqzZEpKyJXdoExC3yRAGnZj8Sn4f8\noqlMgTa3240FCxZg0KBBGDx4MLZu3Sr1miqOSIh00DAikan9u2PHDly4cAG5ubmorKxESkoK+vTp\nA6fTWe9rqjgiIdLBZIEsmZKdMgXacnJyMHr0aNjtdkRGRmLQoEHYtWuXz9dUcURCpIPbDdyRf2hv\n7NixKC72XqaeMWMGXnnF3P1KLpcLMTExnu+jo6M91fTqe00VEwmRDiZXbTZt2gT3fU8Ct2rVSkdk\nWjCREOlgco7EqpKd0dHRKCkpQY8ePQB4j0Lqe00V50iIdDA5R2KVoUOHYuvWrTAMA+Xl5cjLy0NS\nUpLP11RxREKkgxCST//KL//K1P4dMWIEjh49iiFDhgAApk+fjtjYWACo9zVVUonk3LlzyMjIQGVl\nJSIiIpCZmYm4uDhLAiAKShrubJ0zZw7mzJnzwM/vLdTucDiwYMGCWt9f32uqpC5t5s2bhzFjxmD3\n7t0YM2aMV6VzIqpFzRyJzBEEfCaSsrIyFBQUIDk5GQCQnJyMgoIClJeXaw+OKGC53fJHEPCZSFwu\nF6Kiojy7TDkcDrRv396r8jwR3admjsTXEST7ijXIZGvzNp2t6addF+U+Fp/fbEEk1vVjlX+WfOzn\nCLxdLP/S3yF4lH99puFP2sQe2vOZSKKjo3Hp0iW43W44HA643W5cvnzZ1Lr37bLzyvsuNG/XBbf/\n96xSHwAwt+eDk1VmLT6/GbM7v6jcDwB8fFt9ZPfPko/RJ6a/cj9WbWx0sfxLxEZ+T7kfKzY2Kv/6\nDCJbPqHUR2ynjjh64mNzb2pi2wj4vLRp06YNEhISsHPnTgDAzp07kZCQgMjISO3BEQWsJjbZKnVp\nM3/+fGRkZGDVqlVo1aoVMjMzdcdFFNgE5OY/gmOKRC6RdOnSxdK9C4iCXhMrR8E7W4l0cN+RLNnJ\nPVuJqA7CEBCGxHWLTJsAwERCpIOGZ20aMyYSIh2a2PIvEwmRDoaQu2zhpQ0R1Yl3thKRMkNyjoQj\nEiKqkyH5ZK8RHE//MpEQ6cA5EiJSxlUbIlKmaUQis+3p66+/jlOnTnm+P3XqFLKysjBw4ECsWLEC\n7733Htq3bw8AeOaZZzBv3jxTMdSGiYRIAyEEhIYb0mq2PR0xYgT+/ve/Y+7cudi4caNXmyVLlni+\nPnnyJF5++WX069fP87OUlBSkp6ebOq8vDZJIrv/+DYiyS0p9hL+7B9fSf64cy+zBNuU+qvuxZu+O\n07nWFEGKaabeTwms2z7TDvVf53/fuGpBJOr9RNy8Zv5NJkckLper1gJZ9xbJqtn2dP369QCqtz1d\ntGgRysvL69zW429/+xuGDRuGkJAQ85/BBI5IiHQwOUciU7Kzvm1Pa0skt27dwo4dO7Bhwwavn3/w\nwQfYv38/2rVrh1deeQXf//73TX64BzGREOngdgN3JJZ2745CdJTszMvLQ0xMDBISEjw/e+GFF/Dz\nn/8czZs3R35+PtLS0pCTk4PWrVsrnYuJhEgHA5KXNtX/k9m61Oy2p9u2bcOoUaO8ftauXTvP188+\n+yyio6Nx5swZ9OrVy3es9WDJTiIdNJTsNLPtaWlpKb744gsMGzbM6+eXLn07V/nVV1+huLgY3/nO\ndx7yQ36LIxIiHTQt/9a17em9ZTsBYPv27RgwYADCw8O93r906VKcOHECdrsdzZs3x5IlS7xGKQ+L\niYRIA2EYcsu/Jh/aq2vb03vLdgLAtGnTan2/rv2WmUiIdBCSIxJubEREdXIbcg/tuXmLPBHVhQ/t\nEZGq6lvkeWlDRCo4IiEiZdxqkYiUcURCRMoEJJd/tUfSIHzeIp+ZmYnExETEx8fj9OnTDRETUcAT\nbkP6CAY+E8nAgQOxadMmdOzYsSHiIQoONZc2MkcQ8Hlp07Nnz4aIgyiosPavBq3+68+W9BP+7h5L\n+rFCq3dyLelnmyW9ANsu/MOinqxxvvyYv0PwcN8uafiT8hZ561W9Ns6SrRb/PXGwciw2u/oWgK3e\nyUXV5CHK/QDAhNxQ5T62XfgHRnUartzPf39TqNwHUJ1EOkf2UO6n6Osryn24b5fA0TxGqY/OnZ34\nn//3ubk3GfDsNeKzXRDgqg2RDpKXNjZe2hBRnZrYfSQ+V23efPNN/OhHP0JpaSkmTJiA559/viHi\nIgpowg2IO8L3ERwVO32PSObMmYM5c+Y0RCxEwYNzJESkSnb5V2qJOAAwkRDpICA32giOPMJEQqSD\nrhriMrV/66vve/36dcyePRsnTpyAw+FAeno6BgwYYC6IWjCREOmgaY5EpvYvUHd933Xr1iEsLAx7\n9uxBYWEhxo4di9zcXDz22GPmArkP69oQaWC2rI3L5UJRUZHXUVVV5dVnTe3f5ORkANW1fwsKClBe\nLl+z+cMPP0RqaioAIC4uDt27d8enn36q/Hk5IiHSoHr5V64dYH3t37rq+5aUlHg9gBsdHY3S0tKH\n+YhemEiINDA7R2Jl7V9d9X3rw0RCpIHZRGJl7d/66vvGxMSguLjYM4JxuVzo3bu3/AerA+dIiLSw\nAULigPxDpLK1f+ur7zt06FBs2bIFAFBYWIgvv/wS/fr1U/ysHJEQ6SFbH9zkqo1M7d/66vtOmjQJ\nGRkZGDx4MOx2OxYuXIiwsDCTH+5BTCREGlRf2vgebZi9j0Sm9m999X1btGiB5cuXmzupBCYSIg10\n3ZDWWDVIIsn/OBLXLqr9io0CkJfTzmc7X65asLHReADZFsQCAJ1DrHn8s7O9hXIfkS27WhBJtSEW\n9HWtZRcLIgFeiFGbTGwbZf732jBsMNy+/6zZJEYtgYAjEiINhGGTvLRhIiGiOgghtx1rkGzZykRC\npANHJESkTsglkup7SQIfEwmRBry0ISJlhtsOw+37xnGZNoGAiYRIA95HQkTKDGGDITH/IdMmEDCR\nEGkgAAiJJBEkUyRMJERaSC7/gsu/RFQXrtoQkTLekEZEytyGHW7D99KuXaJNIGAiIdKgerJVrl0w\n8JlIKioq8Prrr+PChQsICQlB586dsXDhwge2dyOibwnJ5V+ZlZ1A4HNcZbPZMHnyZOzevRs7duxA\nbGws/vCHPzREbEQBSwib9BEMfI5IIiIivHaZfvrpp7F582atQREFOl2rNjIlO7OyspCTk+PZs3Xm\nzJmeDZ4zMjJw4MABT2mKoUOHYtq0aeaCqIVNCPmPYhgGJk6ciMTERIwfP1755ETB6tgPp+JW0WWf\n7UKc7dHjs7XS/Y4fPx6jRo3ylOzctm3bAyU79+3bh549e+LRRx/FyZMnMW7cOOzfvx+hoaHIyMhA\n9+7dMW7cONOfqT6mJlsXLVqEFi1amA7iwx/8AtcuXjH1nvuNKn0P2zqMUeoDsGirxZJN2BgzVrkf\nADhiwVaLSwv/gllxLyj387VMaTgJb5//G6Z0/k/lfq5B/ddm0/ntGNt5pFIfbZ3tsCxf/i87UDMi\nkZkjqf6/y+WqtUDWvUWyakp2rl+/HkB1yc5FixahvLzca87y3vIS8fHxEEKgsrISHTp0MPUZzJBO\nJJmZmTh//jzWrFkDuz04lqyIdDGEDW4Tz9pYXbKzRnZ2Njp16uSVRNavX48tW7YgNjYWv/rVr9Cl\ni/reuFKJZOnSpTh+/DjWrl2LkJAQ5ZMSBTuzD+1ZWbKzxueff45ly5bh3Xff9fxs5syZaNeuHex2\nO7KzszF58mTk5eV5ktPD8plIzpw5gz/96U+Ii4vDCy9UD5+dTieysrKUTkwUzATkVmTE3Up7Vpbs\nBIDDhw/jtddew6pVq/D44497fh4VFeX5OiUlBYsXL0ZpaalXYfGH4TORPPHEEzh16pTSSYiaGgNy\nRfTMbEdyb8nOESNG1Fmy89ixY5g5cyaWL1+Obt26eb126dIlTzLZt28f7Ha7V3J5WLyzlUgDAZtn\ntOGrnRkyJTsXLFiAGzduYO7cuZ73LVmyBPHx8UhPT0dZWRlsNhvCwsKwevVqNGumngaYSIg0EAIw\nNNxHIlOyc9u2bXW+f8OGDeZOKImJhEgDAzYYEqMNmTaBgImESAM3bHBLJAmZNoGAiYRIA11zJI0V\nEwmRBgJyKzJNZhsBIjJPx/JvY8ZEQqSBgNxlC0ckRFQnwya3QXyQbNnKREKkA5d/iUiZAUhtgsA5\nEhOKmttRFaK+9UChBX20sOh3zop9TQDAbdFVshX9RKK5BZFY11eERfHEIFTp/a3xiOn3GLDBsHFE\nQkQKBOQmUjnZSkR14vIvESkTkqs2QbKJPBMJkQ5ctSEiZW5b9SHTLhgwkRBpwGdtiEgZV22ISFlT\nu0WeBWqINDBMHGacO3cOqampSEpKQmpqKgoLCx9o43a7sWDBAgwaNAiDBw/22pqxvtdUcERCpIGu\n+0jmzZuHMWPGeEp2zp0794GSnTt27MCFCxeQm5uLyspKpKSkoE+fPnA6nfW+poIjEiIdbNX3iPg6\nzKz+1pTsTE5OBlBdsrOgoADl5eVe7XJycjB69GjY7XZERkZi0KBB2LVrl8/XVHBEQqTBnbuHTDtA\nrvavbMlOl8uFmJgYz/fR0dEoLS31+ZoKJhIiDcyu2sjU/m3MmEiINDC7aiNT+1e2ZGd0dDRKSkrQ\no0cPAN6jkPpeU8E5EiINam5I83XUjEiio6PhdDq9jvsTyb0lOwHUWbJz6NCh2Lp1KwzDQHl5OfLy\n8pCUlOTzNRVSI5K0tDQUFRXBbrejRYsW+O1vf4uEhATlkxMFK12rNjIlO0eMGIGjR49iyJAhAIDp\n06cjNjYWAOp9TYVUIsnMzETLli0BAHl5efjNb36D7du3K5+cKFjpurNVpmSnw+HAggULan1/fa+p\nkEokNUkEAL755hvYJHZ+ImrKmtqdrTYh5MoYv/HGG8jPz4cQAu+88w6eeOIJ3bERBaxVz/4S/y66\n4rNduLMt0vL/2AAR6SW9avPWW28BALKzs7FkyRKvoZQvb/f9JaokflHr86sLf8b/6TROqQ/Amj1b\npxX9Gaud6rEAwMlmt5X7WFa4Ba/GpSr3EyqsmXvPPL8Z6Z1fVO7Hit3D/uv8ZrymGEtrZ1v8Jn+F\nqfcYEDAkLlxk2gQC039yUlJScPDgQVRUVOiIhygo6HrWprHymUiuXr0Kl8vl+X7v3r0IDw9HRESE\n1sCIApkwcQQDn5c2169fx6uvvorr16/DbrcjPDwca9as4YQrUT24sdF92rZti7/+9a8NEQtR0Ghq\nqza8RZ5Ig6Y22cpEQqSBG3IlO2XaBAImEiINhOSIRHBEQkR14ebPRKSMJTuJSFl1IpGZbA0OTCRE\nGvDShoiU8YY0IlLmhoBbIk3ItAkETCREGvCGNCJSxjkSDeyi+rCiH1U3LHq2wap+rJq1t6IfK3cC\nt6Kv5maqR9XjEcV+Qh7i/bwhjYiU+eM+kuvXr2P27Nk4ceIEHA4H0tPTMWDAgAfa5eXlYdWqVbh1\n6xaEEBg1ahQmTpwIAHj//ffxu9/9Dh07dgQAOJ1OZGVl+Tw3EwmRBuLufzLtrLJu3TqEhYVhz549\nKCwsxNixY5Gbm4vHHnvMq127du2wevVqREVF4euvv8bPfvYz9OjRAz179gQA9O3bF8uXLzd1bta1\nIdKgZtVG5gCqC1UVFRV5HVVVVabO+eGHHyI1tXrLzbi4OHTv3h2ffvrpA+2eeuopREVFAaje2L1L\nly4PVPkziyMSIg3M3kdiRcnOkpISzyUJIFfX9+zZszhy5IhXiYrPP/8cI0aMQFhYGKZMmYL+/fv7\nPDcTCZEGhhAwJAo01LSRKdk5cuRIlJSU1NrPgQMHTMd4+fJlpKWlYd68eZ4RSv/+/fHTn/4UoaGh\nKCgowJQpU7Bx40Z06dKl3r6YSIg0MLv8e3/93tr4KkoXExOD4uJiTwlPl8uF3r1719q2rKwMEyZM\nwOTJk/GTn/zE8/N7y38++eSTeOaZZ3Ds2DGfiYRzJEQa1NyQJnNYZejQodiyZQsAoLCwEF9++SX6\n9ev3QLuKigpMmDABY8eOxejRo71eu3Tpkufr4uJiHDlyBPHx8T7PzREJkQbVIxKZVRvrTJo0CRkZ\nGRg8eDDsdjsWLlyIsLAwAMCyZcvQvn17vPjii1i7di0KCwuxZcsWT+IZP348Ro0ahU2bNuGjjz6C\nw+EAAMyaNQtPPvmkz3MzkRBp4I+H9lq0aFHnsu2rr77q+To9PR3p6em1tps1axZmzZpl+txMJEQa\nuGHALZFKZNoEAiYSIg24QxoRKRNCQEgs/8q0CQRMJEQa8KE9IlLW1C5tTN1HsnLlSsTHx+P06dO6\n4iEKCsLEf8FAekRy4sQJHDlyxOtefiKqXVPbIU1qRHLr1i0sXLgQ8+fP1xwOUXAwhIBb4pB5HicQ\nSI1Ili1bhuHDh8PpdOqOhygo+OPOVn+yCR/rT4cPH8Yf//hHbNiwATabDYmJiVizZg26du3aUDES\nBZyxfcbjUtEln+2inFHY9M+NDRCRXj5HJIcOHcLZs2cxcOBAAEBpaSkmTZqExYsX47nnnpM6ybo+\nv0RV0RWlQGde/DP+b+w4pT6sYmUs/+O4rdzHisIteCUuVbmfMGHNM5yLz2/G7M4vKvfjsGDP1jfP\nv4c5ncco9RHhbItf55vbMYz3kdxn6tSpmDp1qud7jkiIfGtqk628j4RIC9ml3SaaSPbu3asjDqKg\nYnaHtEDHEQmRBizZSUTKOEdCROqE5IpMcOQRJhIiHTgiISJl/qi0J1uy8+DBg5g6dSri4uIAACEh\nIdi6davn9aysLM+O9SNHjsT06dN9npuJhEgDf9yQJluyEwC6dOmC999//4GfHzp0CLt27cLOnTsB\nAKNHj0avXr3wgx/8oN5zsxwFkQb+KEchW7KzPjk5OUhJSUFoaChCQ0ORkpKCnJwcn+/jiIRIAwMG\n3ML3tkXG3a2NXC5XrZX27q+2Vx8zJTsLCwsxcuRINGvWDGPGjMHIkSM9cfTq1curj0OHDvk8NxMJ\nkQZm50hkav9aVbKzW7du+OSTT9CyZUtcvHgREyZMQFRUFPr27Svdx/2YSIg0MITcXavG3SYytX+t\nKtlZUzQLAGJjYzFo0CD861//Qt++fREdHe2VrFwul1Q5Uc6REGlgdqvF6OhoOJ1Or8PMZQ0gX7Lz\n8uXLnkneyspK5Ofn47vf/a6nj+zsbNy4cQM3btxAdna2V23gujTIiCSsQ6TvRhJaOdta0o8VrIol\n0nHHmn6c7ZT7aCHUH9uvEWHBr48V2wgA6rG0eog/v/541ka2ZGdubi42b96MZs2awe12IyUlBYMG\nDQIA9O7dG0OGDMHzzz8PAEhJSfGaM6mLz42NiMi8gf8xHMUXXT7bdYyNxkdf/KMBItIrIC5tXC4X\nEhMT4XL5/o1pSrEAjKexxlIzIpE5gkFATLa63W4UFxc/MBnV1GMBGE9jjcUt5JZ/ZdoEgoBIJESB\nxh+3yPsTEwmRBtW3yPsebQTLFCUTCZEGrP3bCLVq1QozZswwva4e7LEAjKexxtLUdpHn8i+RBj98\nagiKLtZ+O/u9nLEx+OxobgNEpFdAjEiIAo1bGHAbXLUhIiUsR0FEioTknq3BMrHAREKkAfdsJSJl\nTW3VhomESANW2iMiZRyREJEyQ/KhPYPLv0RUF17aEJEyAbnnaIIjjTCREGnBEQkRWUBusjVYxiRM\nJEQacGMjIlLG5V8iUhYT00Fq/iMmpkMDRKMf9yMhImUBUY6CiBo3JhIiUsZEQkTKmEiISBkTCREp\nYyIhImX/H/KY3pvI1QNXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([24, 41, 15, 1, 40], [24, 41, 0, 0, 0], [24, 41, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ry7S5fvYkvp"
      },
      "source": [
        "# **Revision Self Attention Model 4 (5000 epochs)**\n",
        "\n",
        "1.   Adjust the hidden neurons number of query, key and value's layers\n",
        "2.   Concatenate the original input to the attention layer with the attention output (attention map)\n",
        "3.   Add the layer normalization\n",
        "4.   Same result as model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgcD04wGYkvs",
        "colab": {}
      },
      "source": [
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# train and evaluate a model, return accuracy\n",
        "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs):\n",
        "\t# train LSTM\n",
        "\tfor epoch in range(epochs):\n",
        "\t\t# generate new random sequence\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\t# fit model for one epoch on this sequence\n",
        "\t\tmodel.fit(X, y, epochs=1, verbose=0)\n",
        "\t# evaluate LSTM\n",
        "\ttotal, correct = 100, 0\n",
        "\tfor _ in range(total):\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\tyhat = model.predict(X, verbose=0)\n",
        "\t\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\t\tcorrect += 1\n",
        "\treturn float(correct)/float(total)*100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "427b6c46-8817-4e90-9cd7-3e7928b3018c",
        "id": "OH0ed5avYkvv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "# My own attention model\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        " input_ = Input(shape=(n_timesteps_in, n_features))\n",
        " x = LSTM(150, return_sequences=True)(input_)\n",
        "\n",
        " # Create query, key, value for self-attention computation\n",
        " # Conv1D with kernel_size 1 is equal to Dense\n",
        " query = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " key = Conv1D(filters=75, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " value = Conv1D(filters=150, kernel_size=1, activation=\"relu\", strides=1)(x)\n",
        " # Dot product query and key - the query_key's size should be (timesteps * timesteps)\n",
        " # axes: indicate the axes to multiply with the other tensor\n",
        " query_key = Dot(axes=2, normalize=True)([query,key]) \n",
        " # Softmax is applied across the columns to find the attention of one timestep to the others\n",
        " # The attention's size should be (timesteps * timesteps)\n",
        " attention = Activation(\"softmax\")(query_key)\n",
        " attention = LayerNormalization()(attention)\n",
        " content_vector = Dot(axes=1, normalize=True, name=\"reg_dot2\")([attention,value])\n",
        " content_vector = concatenate([x,content_vector])\n",
        " output_ = Dense(n_features, activation=\"softmax\")(content_vector)\n",
        "\n",
        " model = Model(inputs=input_, outputs=output_)\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# configure problem\n",
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "n_repeats = 5\n",
        "epochs = 5000\n",
        "\n",
        "# evaluate encoder-decoder with attention model\n",
        "print('Encoder-Decoder With Attention Model')\n",
        "results = list()\n",
        "for i in range(n_repeats):\n",
        "  model = attention_model(n_timesteps_in, n_features)\n",
        "  if i == 0:\n",
        "    model.summary()\n",
        "  accuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features, epochs)\n",
        "  results.append(accuracy)\n",
        "  print(accuracy)\n",
        "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder-Decoder With Attention Model\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 5, 150)       120600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 5, 75)        11325       lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 5, 75)        11325       lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_6 (Dot)                     (None, 5, 5)         0           conv1d_18[0][0]                  \n",
            "                                                                 conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 5, 5)         0           dot_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 5, 5)         10          activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 5, 150)       22650       lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reg_dot2 (Dot)                  (None, 5, 150)       0           layer_normalization_6[0][0]      \n",
            "                                                                 conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 300)       0           lstm_6[0][0]                     \n",
            "                                                                 reg_dot2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 5, 50)        15050       concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 180,960\n",
            "Trainable params: 180,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "100.0\n",
            "100.0\n",
            "100.0\n",
            "100.0\n",
            "100.0\n",
            "Mean Accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8-zKfPY4Ykv0"
      },
      "source": [
        "**Activation Map Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b938a35-9311-4079-a82c-cf672565966c",
        "id": "EzKbgrEZYkv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "# For revision model 4\n",
        "activation_model = Model(inputs=model.input, outputs=model.layers[-5].output, name=\"Activation_model\")\n",
        "activation_model.summary()\n",
        "\n",
        "X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "activation = activation_model.predict(X, verbose=0)\n",
        "yhat = model.predict(X, verbose=0)\n",
        "\n",
        "fig = plt.matshow(activation[0])\n",
        "plt.colorbar(fig)\n",
        "plt.show()\n",
        "\n",
        "one_hot_decode(X[0]), one_hot_decode(y[0]), one_hot_decode(yhat[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Activation_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 5, 50)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 5, 150)       120600      input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 5, 75)        11325       lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 5, 75)        11325       lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_10 (Dot)                    (None, 5, 5)         0           conv1d_30[0][0]                  \n",
            "                                                                 conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 5, 5)         0           dot_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 5, 5)         10          activation_10[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 143,260\n",
            "Trainable params: 143,260\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAADwCAYAAADMzOseAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIsOYSIjIMCYSIjKMiYSIDGMiISLD/j+hPpZKc+BbEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([20, 11, 47, 2, 21], [20, 11, 0, 0, 0], [20, 11, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8jO9DQ0arcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}